## Symmetric Quantization 对称量化

将绝对值最大的值的绝对值的负值即$-max(|X_f|)$映射到-128，正值$max(|X_f|)$映射到127，故有一个半轴是没有充分利用的。

scale的计算公式：

$$scale = \frac{x}{2^{num\_bits}-1}\quad if \quad min(x)>0 \quad scale=\frac{x}{2^{num\_bits-1}-1}$$

缺点：低比特利用率。

优点：对于ReLU，比特利用率较高。


## Pytorch Quantization

float模型流程：

x => conv => output

int模型流程：

x => Qx => Qconv => DQoutput


输入量化：

$$ Qx = roundclip(\frac{x}{scale_x}) $$

卷积层量化：
$$ conv_{output} = roundclip(Qx * roundclip(\frac{w}{scale_{wt}})+roundclip(\frac{b}{scale_x*scale_{wt}})) $$

输出映射回float：
$$output = conv_{output} * scale_{out}$$


以上scale均为统计得到。


## torch fx

Problems:
1. eager mode, no directed acyclic graph(DAG).
2. lack of flexible api to modify model.

torch fx:
1. fx is a toolkit for developers to use to transform nn.Module
2. three components: symbolic(符号) tracer; intermediate representation(graph); python code generation(code)

### GraphModule

1. GraphModule inherets nn.Module
2. GraphModule has graph and code,(auto gen)
3. graph can be iterated to get node, and node correspond to raw module or function or method.
4. GraphModule has same modules and state_dict as raw models.
5. GraphModule can be deployed without model python file.



scale_act：输入的尺度

scale_wt：权重的尺度，conv的话是逐通道的

scale_out：输出的尺度

1. prepare qconfig for each node;
2. insert observer for each node, observer is configured by qconfig;
3. run calibration(activation quantization and weight&bias quantization);
4. convert float modules to quantized modules(Conv->QConv);
5. evaluate quantized model;
6. profile model;

## Tracer
```
from torch.fx import Tracer, Graph, Node

class BrocolliTracer(Tracer):
    def is_leaf_module():
        pass
```

自定义BrocolliTracer继承了torch.fx的Tracer，is_leaf_module用于判断是否为叶子节点，从而防止将一个大算子（如LayerNorm）拆分成几个小算子。

